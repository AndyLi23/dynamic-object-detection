<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}

	.main-content-block {
		width: 80%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 30px 8px 30px;
		font-family: 'Garamond', serif;
		line-height: 1.5;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Garamond', serif;
			line-height: 1.5;
			padding: 5px;
	}
	.margin-right-block {
			font-family: 'Garamond', serif;
			line-height: 1.5;
			font-size: 14px;
			width: 5%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>FLOROS</title>
      <meta property="og:title" content="FLOROS" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Garamond', serif;">Flow Residuals for Open Set dynamic object tracking (FLOROS)</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:21px"><a href="your_website">Qingyuan Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:21px"><a href="your_partner's_website">Juan Rached</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">MIT 6.8300 Final Project</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#related_works">Related Works</a><br><br>
              <a href="#methods">Methods</a><br><br>
              <a href="#experiments">Experiments</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
              <a href="#citations">References</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
			<p>
				Advances in autonomous Unmanned Aerial Vehicle (UAV) navigation are improving our quality of life. The integration of this technology into emergency services, 
				search and rescue, first responders, as well as law-enforcement, and agriculture has observed accelerated growth in the past decade, promising a safer future.
				UAV navigation has also had an impact in the delivery industry. While this might be perceived as superfluous next to the former, UAV deliveries enable the prompt 
				dispatch of sensible products, such as medicine, to remote communities, difficult to access through ground transportation. This is still, however, an early technology,
				with several limitations and open research questions. In particular, dynamic obstacle avoidance for UAVs is a longstanding problem in robotics.
			</p>
			<p>
				Current state-of-the-art methods rely on event cameras, stereo cameras, depth cameras, or lidar.
				But power and payload limitations restrict the perception and compute capacity of a UAV. These constraints give rise to a hard-to-strike 
				balance between latency and performance for real-time applications that is still an open question in the field today. 
				Some methods favor efficiency while some favor performance, but the field is yet to adopt a general procedure to estimate the motion of dynamic obstacles. 
				As a consequence, a large number of path planning algorithms assume static environments, but this is a 
				strong assumption as we live in a dynamic world. Some methods perform dynamic obstacle avoidance by assuming the state of the dynamic obstacle is known at all times. 
				This is fine in a laboratory setting, where a motion capture system can provide such estimates, but breaks in real world applications. So a scheme for estimating the motion 
				of dynamic obstacles from sensory input is indispensable if we are to deploy a complete system. Some approaches employ Kalman Filters to recover the position and velocity of the obstacle in the field 
				of view from a point cloud. But this is either restrictive, allowing robust planning only around vehicles with known dynamics, or brittle, if nothing is assumed about the obstacles' dynamics. 
			</p>
			<p>
				There are two core challenges in estimating the motion of objects from a moving platform:
				<ol>
					<li>Estimating the motion of the scene.</li>
					<li>Removing the motion of the vehicle attached to the camera.</li>	
				</ol>
			</p>
			<p>
				This becomes increasingly difficult when performing highly dynamic, high-speed motion as is the norm in the UAV literature. At these speeds,
				motion blur pollutes the visual signal, which complicates the task of separating agent from obstacle motion. In this project, we propose the use of RAFT optical flow and the optical 
				flow induced by scene flow to solve both of the aforementioned problems with just a frame camera.
			</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="related_works">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
					<h1>Related Work</h1>
		<p>
			In <a href="#ref_1">[1]</a>, Tordesillas et al, propose a fast and safe trajectory planner that plans in known and unknown environments. In other words, it plans within the observed and 
			unobserved regions of the map. But this approach assumes a static world. In <a href="#ref_2">[2]</a> and <a href="#ref_3">[3]</a> present a multiagent, asynchronous, and decentralized trajectory
			planner and an extension to mitigate delay checks in hardware deployment. While both <a href="#ref_2">[2]</a> and <a href="#ref_3">[3]</a> can plan in dynamic environments, they assume that the state of the 
			dynamic objects is known. <a href="#ref_4">[4]</a>, a real-time perception-aware trajectory planner for multirotor UAVs, can plan in dynamic environments and does not expect 
			the state of dynamic objects to be given. Instead, <a href="#ref_4">[4]</a> clusters the point clouds produced by an onboard depth sensor and applies the Hungarian algorithm to track 
			those clusters. <a href="#ref_4">[4]</a>, however, does not work in static scenes, and its point cloud scheme breaks when static objects are picked up by the depth sensor as well. 
		</p>
		
		<p>		
			<a href="#ref_5">[5]</a> propose the use of event cameras for low-latency dynamic obstacle detection and avoidance.
			While they achieve faster response times than frame cameras, event cameras are expensive and do not capture the full scene. 
			Since they cannot pick up static objects or color, they are incapable of performing tasks relevant to robotics systems such as feature matching, segmentation, among others.
			Frame cameras can be used to solve a variety of perception tasks, but require more elaborate algorithms for motion detection than event cameras. 
		</p>


		<p>
			The fundamental vision-based approach to motion estimation is optical flow, and optical flow methods for UAV applications have been thoroughly studied <a href="#ref_6">[6]</a>. 
			But standard optical flow struggles with fine motion estimation, particularly at high speeds and in dynamic lighting conditions, making them inadequate for UAV navigation. Additionally, 
			pure optical flow methods estimate motion between frames, but capture no semantics about the motion and its corresponding obstacle and so struggle to track the objects' trajectory along a sequence. 
			<a href="#ref_7">[7]</a> presents a transformer-based point tracking pipeline that successfully estimates the trajectory of dynamic objects from medium-to-long video sequences. 
			Estimating obstacle trajectories with point tracking is considerably more robust than with optical flow alone, but <a href="#ref_7">[7]</a> needs a procedure to remove the ego motion of the UAV from the point tracks. 	
		</p>

		<p>
			<a href="#ref_8">[8]</a> leverages the scene flow induced by a moving camera to produce high-quality camera poses, intrinsics and depth estimates. 
			We propose reversing the pipeline from <a href="#ref_8">[8]</a>, computing the expected optical flow between two frames given depth data, camera poses, and intrinsics, under the assumption of a static scene. 
			If there are any non-static objects in the scene, we can subtract the expected optical flow from the RAFT-estimated optical flow, <a href="#ref_9">[9]</a> , to obtain an estimate of the motion of the scene not arising from ego motion.
			We will develop an algorithm to convert the residuals into a discrete list of moving objects and their masks. We propagate the masks between frames to obtain the tracks for the dynamic obstacles over time. 

		</p>
		
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Methods</h1>

			<p>
				We compute the residuals from RAFT optical flow and the geometric optical flow induced by scene flow, using solely RGBD and odometry data. The latter assumes a static scene, so 
				the residuals substract the flow produced by the motion of the vehicle, capturing only the movement of the dynamic objects in the scene. Below is a 
				detailed discussion on how each of the modules involved in our approach work and the role they play in the pipeline. 
			</p>

			<h4>Theory</h4>
			<p>
				Optical flow assigns pixel correspondences between two frames by solving the optimization problem below: 
			</p>
			\[ E = \min_{\mu} \left[ (\nabla^T \mathbf{I}_1\mu - (\mathbf{I}_2-\mathbf{I}_1)^2+\alpha(|\nabla \mu_x|^2+|\nabla \mu_y|^2)) \right] \] 
			<p>
				Where the first term enforces correspondences based on the color constancy assumption, which states that the color of each point in the scene does not change between frames, and the second term promotes smooth uniform flow by encouraging each 
				flow vector to be close to the average of its neighboring vectors. Common pitfalls of classic optical flow include: 
			</p>
			<ol>
				<li>It is expensive to solve an optimization problem for each frame in a video sequence.</li>
				<li>The color constancy assumption is susceptible to blur and changes in lighting conditions.</li>
			</ol>
			<p>
				As a consequence, it has become the norm to substitute the classical optical flow approach with learned models, the most notable of which is 
				RAFT. RAFT extracts per pixel features and builds multi-scale 4D correlation volumes. This rich representation allows the model to generalize 
				and reduce the effect of lighting changes and blur on the flow estimates. The compact representation makes it so that computing the flow between 
				each frame is also faster than if solving the original optimization problem. 
			</p>
			<p>
				We obtain our flow residuals by computing the difference between the RAFT optical flow and the "geometric optical flow", or optical flow
				resulting solely from camera-induced scene flow. 
				<br><br>
				For each pixel \( [u, v] \) with corresponding depth \( d \) in the first frame, we first unproject the pixel coordinates into 3D space using the depth data and camera intrinsics:
				\[ p_1=d K^{-1} [u, v, 1]^T \]
				We assume that \( p_1 \) is static in the 3D scene. We transform the 3D point \( p_1 \), which is in the reference frame of the camera at the frame 1, into the reference frame of the camera at the frame 2. Given that 
				the robot pose is \( T_{world}^{r1} \) at frame 1 and \( T_{world}^{r2} \) at frame 2, and that the transform between the odometry frame and camera frame is \( T_{odom}^{cam} \), 
				we can transform the point as follows:

				\[ T_{r2}^{r1} = (T_{world}^{r2})^{-1} \cdot T_{world}^{r1} \]
				\[ p_2 = (T_{odom}^{cam})^{-1} \cdot T_{r2}^{r1} \cdot T_{odom}^{cam} \cdot [p_1^T \dots 1]^T \]

				Finally, we project the point \( p_2 \) into the image plane to get its pixel location in the second frame:
				\[ [u', v'] = [f_x \frac{p_{2,x}}{p_{2,z}} + c_x, f_y \frac{p_{2,y}}{p_{2,z}} + c_y] \]
				Where \( f_x, f_y \) are the focal lengths of the camera and \( c_x, c_y \) are the coordinates of the camera center in the image plane.

				The (@Andy shouldn't this be flipped?) \( [u, v] - [u', v'] \) gives us the geometric optical flow at \( [u, v] \) in frame 1.

				The key observation is the assumption that \( p_1 \) is static in the 3D scene. If \( p_1 \) moves \( \delta = [\delta_x, \delta_y, \delta_z] \) (in the camera reference frame at frame 2) 
				between frame 1 and frame 2, we get the following image projection at frame 2:

				\[ [u'_{dynamic}, v'_{dynamic}] = [f_x \frac{p_{2,x} + \delta_x}{p_{2,z} + \delta_z} + c_x, f_y \frac{p_{2,y} + \delta_y}{p_{2,z} + \delta_z} + c_y] \]

				Then the geometric optical flow at \( [u, v] \) in frame 1 is (@Andy, isn't this the residual?) \( [u'_{dynamic}, v'_{dynamic}] - [u', v'] \).

				Since RAFT estimates the actual observed optical flow from frame 1 to frame 2, it will return an optical flow close to \( [u'_{dynamic}, v'_{dynamic}] - [u, v] \) at \( [u, v] \).
				Then, the residual at that pixel is \( [u'_{dynamic}, v'_{dynamic}] - [u', v'] \).

				Although we could theoretically recover the depth of \( [u'_{dynamic}, v'_{dynamic}] \) from the depth data for frame 2, in practice, noisy depth data and imperfect
				pixel associations make this a noisy process. Instead, we assume that the object maintains constant depth between frames (\( \delta_z = 0 \)) and use the residual to only recover \( [\delta_x, \delta_y] \):

				\[ [\delta_x, \delta_y] = p_{2,z} ([u'_{dynamic}, v'_{dynamic}] - [u', v']) \cdot [\frac{1}{f_x}, \frac{1}{f_y}] \]

				This assumption works reasonably well in practice, although it is not effective at detecting objects moving along the camera Z axis.
				<br><br>
				Finally, given \( v=\| \delta_x, \delta_y \| \) as an estimate for the velocity of the 3D point corresponding to each pixel, we can mask areas of the image with
				\( v \) higher than a set threshold to obtain masks of dynamic objects in frame 1.

				
			</p>
			<h4>Prototype</h4>
			<p>Below is a video of our prototype of this method running on data from Kimera-Multi, <a href="#ref_10">[10]</a>, where the right image shows flow residual magnitudes masked by the velocity threshold. 
				To track the motion of dynamic objects across frames we first fed the residuals to the point tracking algorithm from, <a href="#ref_7">[7]</a>.
				However, this introduces unnecessary computation, as RAFT optical flow is accurate enough to estimate the motion of each obstacle between the current and next frame.
				The details are discussed below.</p>
			<p>
				We also prototyped a method which uses the 3D residual between the optical flow predicted pointcloud and the odometry predicted pointcloud instead. It was much more noisy,
				so we used flow residuals for our final method. See the appendix below for details.
			</p>
			<div style="display: flex; justify-content: center; align-items: center; gap: 0;">
				<video class='my-video' loop autoplay muted style="width: 300px; max-width: 50%; margin: 0;">
					<source src="./videos/kimera/kimera_walk_og.mp4" type="video/mp4">
				</video>
				<video class='my-video' loop autoplay muted style="width: 300px; max-width: 50%; margin: 0;">
					<source src="./videos/kimera/walk_masked_pt.mp4" type="video/mp4">
				</video>
			</div>
			<h4>Final Algorithm</h4>
			<p>
				We implemented the above method in a pipeline that works with standard ROS bags as input data, with flow and residuals computed in batches using PyTorch tensors on GPU 
				for performance. Once velocity magnitudes are computed from residual flow, we apply a threshold to the velocity magnitudes to obtain a binary mask of dynamic objects.
				The threshold can be tuned. Because depth camera noise increases as depth increases, we set the threshold to scale by depth: \( thresh=min\_thresh + z \cdot thresh\_depth\_gain \)
			</p>
			<p>
				Because of the noise in odometry and depth data, we improved the performance by post-processing the binary mask with OpenCV morphological operations.
				Opening removes small points of noise, while closing removes small holes in the mask.

				Once we have extracted a processed binary mask for the frame, we use scipy.ndimage.label to label connected components. Each connected component is extracted
				as a mask and assumed to be a dynamic object. The pixels in the mask are unprojected to get a 3D pointcloud representing the dynamic object. We discard 
				objects with a maximum axial variance outside a set range to further remove noise.
			</p>
			<p>
				To facilitate tracking, we compute frame-by-frame associations between currently tracked dynamic objects and newly detected dynamic objects.
			</p>
			<p>
				At each iteration, we predict the 3D location of currently tracked dynamic objects by propogating their masks into the next image frame using optical flow,
				then using the next frame's depth data to unproject the masks into predicted point clouds. When dynamic objects are detected in the next frame, we unproject their masks
				in that frame to get detected point clouds. We then use the Hungarian algorithm with point cloud centroid distance-based similarity to match newly detected dynamic objects with 
				currently tracked dynamic objects using their associated point clouds. 
				<br><br>
				We augment the cost matrix and set costs to a large value if the point clouds are more than 
				a set distance apart, so that newly detected objects not close to any currently tracked objects are not matched and become new tracked objects. Otherwise, currently
				tracked objects are updated with their new detections.
			</p>
			<p>A representative video of our algorithm running on data we collected is shown below. Noise is still present, but much of it is filtered out.</p>
				<div style="display: flex; justify-content: center; align-items: center; gap: 0;">
					<video class='my-video' loop autoplay muted style="width: 800px; max-width: 100%; margin: 0;">
						<source src="./videos/hamilton.mp4" type="video/mp4">
					</video>
		    	</div>
			</div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		    </div>
	</div>

	<div class="content-margin-container" id="experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Experiments</h1>
            In this section we discuss our experiments
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="results">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
		<h1>Results</h1>
		In this section, we discuss the outcome of our experiments.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Conclusion</h1>
						Although our method shows promise in efficient real-time detection and tracking of dynamic obstacles, it is extremely sensitive to the quality of data. 
						During our experiments, we observed that poor quality depth data or odometry estimates led to the geometric optical flow having large deviations
						from the RAFT optical flow in static areas of the image. Future work could include using a trained neural network to segment dynamic objects, since we do not
						use the corresponding RGB image as a segmentation prior for each flow residual frame. A neural network that also uses FastSAM segments or superpixels
						would better leverage all the available data. 
						<br><br>
						A planned extension to this work is to use LiDAR as the source for depth data. This would greatly increase the range and accuracy of depth data, at
						the cost of sparsity (which could be addressed with stereo camera-LiDAR fusion). In addition, it may allow us to use the 3D residual method discussed in the appendix,
						which can better detect movement in the camera Z axis.
						<br><br>
						A major limitation of this method is that under high accelerations, the RAFT and geometric flow significantly diverge, leading to underwhelming results during UAV flights. 
						This might be attributed to RAFT not generalizing well to high speed camera motion. Performing transfer learning on RAFT  with labeled high speed flight data 
						is a promising direction of future work. 
						<br><br>
						Despite these shortcomings, our method proves to be a promising approach to the problem of dynamic obstacle tracking. With only odometry and depth camera data, 
						it demonstrates good performance and runtime on real-world data in certain environments. There appear to be many directions for further work to
						improve robustness, efficency, and generalization.
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://ieeexplore.ieee.org/abstract/document/8968021">Tordesillas, J., Lopez, B. T., & How, J. P. (2019, November). Faster: Fast and safe trajectory planner for flights in unknown environments. In 2019 IEEE/RSJ international conference on intelligent robots and systems (IROS) (pp. 1934-1940). IEEE.</a><br><br>
							<a id="ref_2"></a>[2] <a href="https://ieeexplore.ieee.org/abstract/document/9490372	">Tordesillas, J., & How, J. P. (2021). MADER: Trajectory planner in multiagent and dynamic environments. IEEE Transactions on Robotics, 38(1), 463-476.</a><br><br>
							<a id="ref_3"></a>[3] <a href="https://ieeexplore.ieee.org/abstract/document/10161244">Kondo, K., Tordesillas, J., Figueroa, R., Rached, J., Merkel, J., Lusk, P. C., & How, J. P. (2023, May). Robust MADER: Decentralized and asynchronous multiagent trajectory planner robust to communication delay. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1687-1693). IEEE. </a><br><br>
							<a id="ref_4"></a>[4] <a href="https://ieeexplore.ieee.org/abstract/document/9721028">Tordesillas, J., & How, J. P. (2022). Panther: Perception-aware trajectory planner in dynamic environments. IEEE Access, 10, 22662-22677. </a><br><br>
							<a id="ref_5"></a>[5] <a href="https://www.science.org/doi/full/10.1126/scirobotics.aaz9712">Falanga, D., Kleber, K., & Scaramuzza, D. (2020). Dynamic obstacle avoidance for quadrotors with event cameras. Science Robotics, 5(40), eaaz9712. </a><br><br>
							<a id="ref_6"></a>[6] <a href="https://ieeexplore.ieee.org/abstract/document/6564752">Chao, H., Gu, Y., & Napolitano, M. (2013, May). A survey of optical flow techniques for UAV navigation applications. In 2013 International Conference on Unmanned Aircraft Systems (ICUAS) (pp. 710-716). IEEE. </a><br><br>
							<a id="ref_7"></a>[7] <a href="https://link.springer.com/chapter/10.1007/978-3-031-73033-7_2">Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., & Rupprecht, C. (2024, September). Cotracker: It is better to track together. In European Conference on Computer Vision (pp. 18-35). Cham: Springer Nature Switzerland. </a><br><br>
							<a id="ref_7"></a>[7] <a href="https://link.springer.com/chapter/10.1007/978-3-031-73033-7_2">Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., & Rupprecht, C. (2024, September). Cotracker: It is better to track together. In European Conference on Computer Vision (pp. 18-35). Cham: Springer Nature Switzerland. </a><br><br>
							<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2404.15259">Smith, C., Charatan, D., Tewari, A., & Sitzmann, V. (2024). Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent. arXiv preprint arXiv:2404.15259.</a><br><br>
							<a id="ref_9"></a>[9] <a href="https://link.springer.com/chapter/10.1007/978-3-030-58536-5_24">Teed, Z., & Deng, J. (2020). Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16 (pp. 402-419). Springer International Publishing. </a><br><br>
							<a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/abstract/document/9686955">Tian, Y., Chang, Y., Arias, F. H., Nieto-Granda, C., How, J. P., & Carlone, L. (2022). Kimera-multi: Robust, distributed, dense metric-semantic slam for multi-robot systems. IEEE Transactions on Robotics, 38(4). </a><br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
	</div>

	<div class="content-margin-container" id="appendix">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
					<h1>Appendix</h1>
					<h4>Using 3D Residuals </h4>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	</body>

</html>


