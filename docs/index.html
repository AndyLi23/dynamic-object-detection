<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}

	.main-content-block {
		width: 80%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 30px 8px 30px;
		font-family: 'Garamond', serif;
		line-height: 1.5;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Garamond', serif;
			line-height: 1.5;
			padding: 5px;
	}
	.margin-right-block {
			font-family: 'Garamond', serif;
			line-height: 1.5;
			font-size: 14px;
			width: 5%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: 0;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
		padding: 5px 0px;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.caption {
		text-align: center; 
		margin-top: 10px;
		font-size: 16px;
		font-style: italic;
	}

	video {
		margin: 0;
	}

	.media-container {
		display: flex; 
		justify-content: center; 
		align-items: center; 
		gap: 0;	
	}
	
	.underl {
		text-decoration: underline;
		vertical-align: baseline;
	}


	thead tr {
		border-bottom: 2px solid black; /* Makes the line at the bottom of thead thicker */
	}

</style>

	  <title>FLOROS</title>
      <meta property="og:title" content="FLOROS" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left style="margin: 30px 0px 0px 0px;">
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Garamond', serif;">FLOROS: <span class="underl">Flo</span>w <span class="underl">R</span>esiduals for <span class="underl">O</span>pen <span class="underl">S</span>et Dynamic Object Tracking</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:21px"><a href="">Qingyuan Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:21px"><a href="">Juan Rached</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">MIT 6.8300 Final Project</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px); font-size:16px; border: none;">
              <b style="font-size:18px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#related_works">Related Works</a><br><br>
              <a href="#methods">Methodology</a><br><br>
              <a href="#experiments">Experiments</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
              <a href="#citations">References</a><br><br>
			  <a href="#codebase">Codebase</a><br><br>
			  <a href="#appendix">Appendix</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
			<hr style="margin-bottom: 30px;">
			<img src="./images/banner1.png" width=100%/>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
			<p>
				Advances in autonomous Unmanned Aerial Vehicle (UAV) and Unmanned Ground Vehicle (UGV) navigation are improving our quality of life. The integration of this technology into emergency services, 
				search and rescue, first responders, law enforcement, and agriculture has observed accelerated growth in the past decade, promising a safer future.
				UAV/UGV navigation has also had an impact in the delivery industry, where UAV/UGV deliveries enable the prompt 
				dispatch of essential products, such as medicine, to remote communities, difficult to access through typical shipping. However, this is still an early technology,
				with several limitations and open research questions. In particular, dynamic obstacle avoidance for UAV/UGVs is a longstanding problem in robotics.
			</p>
			<p>
				Current state-of-the-art methods rely on event cameras, stereo cameras, depth cameras, or lidar.
				However, power and payload limitations restrict the perception and compute capacity of unmanned vehicles, particularly UAVs. These constraints force a difficult
				balance between latency and performance for real-time applications that is still an open question in the field today. 
				Some methods favor efficiency while others favor performance, but the field is yet to adopt a general procedure to estimate the motion of dynamic obstacles. 
				As a consequence, a large number of path planning algorithms assume static environments, but this is a 
				strong assumption given that we live in a dynamic world. Some methods perform dynamic obstacle avoidance by assuming the state of the dynamic obstacle is known at all times. 
				This is fine in a laboratory setting, where a motion capture system can provide such estimates, but breaks in real-world applications. Therefore, a scheme for estimating the motion 
				of dynamic obstacles from sensory input is indispensable if we are to deploy a complete system. Some approaches employ Kalman filters to recover the position and velocity of obstacles in the field 
				of view from point clouds, but these methods are either restrictive, allowing robust planning only around vehicles with known dynamics, or brittle, if nothing is assumed about the obstacles' dynamics. 
			</p>
			<p>
				There are two core challenges in estimating the motion of objects from a moving platform:
				<ol>
					<li>Estimating the motion of the scene.</li>
					<li>Removing the motion of the vehicle attached to the camera.</li>	
				</ol>
			</p>
			<p>
				Unfortunately, this becomes increasingly difficult when performing highly dynamic, high-speed motion, as is the norm in the UAV literature. At those speeds,
				odometry latency and inaccuracy build up quickly, and motion blur can pollute the visual signal, which complicates the task of separating agent 
				from obstacle motion. In this project, we propose using a combination of RAFT optical flow and the geometric optical 
				flow induced by scene flow to solve both of the aforementioned problems with just a frame camera.
			</p>
            <img src="./images/banner.png" width=100%/>
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="related_works">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
					<h1>Related Work</h1>
		<p>
			In <a href="#ref_1">[1]</a>, Tordesillas et al propose a fast and safe trajectory planner that plans in known and unknown environments. In other words, it plans within the observed and 
			unobserved regions of the map. But this approach assumes a static world. <a href="#ref_2">[2]</a> and <a href="#ref_3">[3]</a> present a multiagent, asynchronous, and decentralized trajectory
			planner and an extension to mitigate delay checks in hardware deployment. While both <a href="#ref_2">[2]</a> and <a href="#ref_3">[3]</a> can plan in dynamic environments, they assume that the state of the 
			dynamic objects is known. <a href="#ref_4">[4]</a>, a real-time perception-aware trajectory planner for multirotor UAVs, can plan in dynamic environments and does not expect 
			the state of dynamic objects to be given. Instead, <a href="#ref_4">[4]</a> clusters the point clouds produced by an onboard depth sensor and applies the Hungarian algorithm to track 
			those clusters. <a href="#ref_4">[4]</a>, however, does not work in static scenes, and its point cloud scheme breaks when static objects are picked up by the depth sensor as well. 
		</p>
		
		<p>		
			<a href="#ref_5">[5]</a> proposes the use of event cameras for low-latency dynamic obstacle detection and avoidance.
			While they achieve faster response times than frame cameras, event cameras are expensive and do not capture the full scene. 
			Since they cannot pick up static objects or color, they are incapable of performing tasks relevant to robotics systems, such as feature matching, segmentation, etc.
			Frame cameras can be used to solve a variety of perception tasks, but require more elaborate algorithms for motion detection than event cameras. 
		</p>


		<p>
			The fundamental vision-based approach to motion estimation is optical flow, and optical flow methods for UAV applications have been thoroughly studied <a href="#ref_6">[6]</a>. 
			But standard optical flow struggles with fine motion estimation, particularly at high speeds and in dynamic lighting conditions, making it inadequate for UAV navigation. Additionally, 
			pure optical flow methods estimate motion between frames, but cannot distinguish individual objects or track them over time.
			<a href="#ref_7">[7]</a> presents a transformer-based point tracking pipeline that successfully estimates the trajectory of dynamic objects from medium-to-long video sequences. 
			Estimating obstacle trajectories with point tracking is considerably more robust than with optical flow alone, but <a href="#ref_7">[7]</a> still would require a procedure to remove 
			ego motion from the point tracks if running on a moving agent. 	
		</p>

		<p>
			<a href="#ref_8">[8]</a> leverages the scene flow induced by a moving camera to produce high-quality camera poses, intrinsics, and depth estimates. 
			We propose reversing the pipeline from <a href="#ref_8">[8]</a>, computing the expected optical flow between two frames given depth data, camera poses, and intrinsics, under the assumption of a static scene. 
			If there are any non-static objects in the scene, we can subtract the expected optical flow from the RAFT-estimated optical flow <a href="#ref_9">[9]</a> to estimate motion in the scene not explained by ego movement.
			We will develop an algorithm to convert the residuals into a discrete list of moving objects and their masks, and will propagate masks between frames to obtain tracks for dynamic obstacles over time. 

		</p>
		
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<hr style="margin-bottom: 30px">
					<h1>Methodology</h1>

			<p>
				We compute the residuals from RAFT optical flow and the geometric optical flow induced by scene flow, using solely RGBD and odometry data. The latter assumes a static scene, so 
				subtracting the flow produced by the motion of the vehicle produces residuals that theoretically capture only the movement of the dynamic objects in the scene. Below is a 
				detailed discussion on how each of the modules involved in our approach works and the role they play in the pipeline. 
			</p>

			<h4>Theory</h4>
			<p>
				Optical flow assigns pixel correspondences between two frames by solving the optimization problem below: 
			</p>
			\[ E = \min_{\mu} \left[ (\nabla^T \mathbf{I}_1\mu - (\mathbf{I}_2-\mathbf{I}_1)^2+\alpha(|\nabla \mu_x|^2+|\nabla \mu_y|^2)) \right] \] 

			<p>
				Where the first term enforces correspondences based on the color constancy assumption, which states that the color of each point in the scene does not change between frames, and the second term promotes smooth uniform flow by encouraging each 
				flow vector to be close to the average of its neighboring vectors. Common pitfalls of classic optical flow include: 
			</p>
			<ol>
				<li>It is expensive to solve an optimization problem for each frame in a video sequence.</li>
				<li>The color constancy assumption is susceptible to blur and changes in lighting conditions.</li>
			</ol>
			<p>
				As a consequence, it has become the norm to substitute the classical optical flow approach with learned models, the most notable of which is 
				RAFT. RAFT extracts per-pixel features and builds multi-scale 4D correlation volumes. This rich representation allows the model to generalize 
				and reduce the effect of lighting changes and blur on the flow estimates. The compact representation makes it so that computing the flow between 
				each frame is also faster than solving the original optimization problem. 
			</p>
			<p>
				We obtain our flow residuals by computing the difference between the RAFT optical flow and the "geometric optical flow", or optical flow
				resulting solely from camera-induced scene flow. 
				<br><br>
				For each pixel \( [u, v] \) with corresponding depth \( d \) in the first frame, we first unproject the pixel coordinates into 3D space using the depth data and camera intrinsics:
				\[ p_1=d K^{-1} [u, v, 1]^T \]
				We assume that \( p_1 \) is static in the 3D scene. We transform the 3D point \( p_1 \), which is in the reference frame of the camera at frame 1, into the reference frame of the camera at frame 2. Given that 
				the robot pose is \( T_{world}^{r1} \) at frame 1 and \( T_{world}^{r2} \) at frame 2, and that the transform between the odometry frame and camera frame is \( T_{odom}^{cam} \), 
				we can transform the point as follows:

				\[ T_{r2}^{r1} = (T_{world}^{r2})^{-1} \cdot T_{world}^{r1} \]
				\[ p_2 = (T_{odom}^{cam})^{-1} \cdot T_{r2}^{r1} \cdot T_{odom}^{cam} \cdot [p_1^T \dots 1]^T \]

				Finally, we project the point \( p_2 \) into the image plane to get its pixel location in the second frame:
				\[ [u', v'] = [f_x \frac{p_{2,x}}{p_{2,z}} + c_x, f_y \frac{p_{2,y}}{p_{2,z}} + c_y] \]
				where \( f_x, f_y \) are the focal lengths of the camera and \( c_x, c_y \) are the coordinates of the camera center in the image plane.

				Then, \( [u', v'] - [u, v] \) gives us the geometric optical flow at \( [u, v] \) in frame 1.

				The key observation is the assumption that \( p_1 \) is static in the 3D scene. If \( p_1 \) moves \( \delta = [\delta_x, \delta_y, \delta_z] \) (in the camera reference frame at frame 2) 
				between frame 1 and frame 2, we get the following image projection at frame 2:

				\[ [u'_{dynamic}, v'_{dynamic}] = [f_x \frac{p_{2,x} + \delta_x}{p_{2,z} + \delta_z} + c_x, f_y \frac{p_{2,y} + \delta_y}{p_{2,z} + \delta_z} + c_y] \]

	
				In the video below, RAFT optical flow is juxtaposed with geometric optical flow. 
				Where the geometric optical flow is white, it cannot be computed because there is no depth data (we capped the
				max depth at 6.0m; see <a href="#app1">Appendix I.</a>).
			</p>
			<div class="media-container">
				<video class='my-video' loop autoplay muted controls style="width: 800px; max-width: 100%;">
					<source src="./videos/flow.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 1. Clockwise from top left: RGB image, depth image, <br>geometric optical flow, RAFT flow</span>
			</div>
			<p>

				Since RAFT estimates the actual observed optical flow from frame 1 to frame 2, it will return an optical flow close to \( [u'_{dynamic}, v'_{dynamic}] - [u, v] \) at \( [u, v] \).
				Then, the residual (RAFT minus geometric flow) at \( [u, v] \) is \( [u'_{dynamic}, v'_{dynamic}] - [u', v'] \).

				Although we could theoretically recover the depth of \( [u'_{dynamic}, v'_{dynamic}] \) from the depth data for frame 2, in practice, noisy depth data and imperfect
				pixel associations make this a noisy process (see <a href="#app2">Appendix II.</a>). Instead, we assume that the object maintains constant depth between frames (\( \delta_z = 0 \)) and use the residual to recover only \( [\delta_x, \delta_y] \):

				\[ [\delta_x, \delta_y] = p_{2,z} ([u'_{dynamic}, v'_{dynamic}] - [u', v']) \cdot [\frac{1}{f_x}, \frac{1}{f_y}] \] 

				This assumption works reasonably well in practice, although it is not effective at detecting objects moving along the camera Z axis.
				<br><br>
				Finally, given \( v=\| \delta_x, \delta_y \| \) as an estimate for the velocity of the 3D point corresponding to each pixel, we can mask areas of the image with
				\( v \) higher than a set threshold to obtain masks of dynamic objects in frame 1.

				
			</p>
			<h4>Prototype</h4>

			<p>Below is a video of our prototype of this method running on data from Kimera-Multi <a href="#ref_10">[10]</a>, where the right image shows flow residual magnitudes masked by the velocity threshold. 
				To track the motion of dynamic objects across frames, we first fed the residuals to the point tracking algorithm from <a href="#ref_7">[7]</a>.
				However, this introduces unnecessary computation, as RAFT optical flow is accurate enough to estimate the motion of each obstacle between the current and next frame.
				The details are discussed below.
			</p>
			<div class="media-container">
				<video class='my-video' loop autoplay muted controls style="width: 300px; max-width: 50%;">
					<source src="./videos/prototype/kimera/kimera_walk_og.mp4" type="video/mp4">
				</video>
				<video class='my-video' loop autoplay muted controls style="width: 300px; max-width: 50%;">
					<source src="./videos/prototype/kimera/walk_masked_pt.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 2. Top is RGB image, bottom is flow residual magnitude <br> masked by the velocity threshold.</span>
			</div>
			<h4>Final Algorithm</h4>
			<p>
				We implemented the above method in a pipeline that works with standard ROS bags as input data, with flow and residuals computed in batches using PyTorch tensors on GPU 
				for performance. Once velocity magnitudes are computed from residual flow, we apply a threshold to the velocity magnitudes to obtain a binary mask of dynamic objects.
				The threshold can be tuned. Because depth camera noise increases as depth increases, we set the threshold to scale by depth: \( thresh=min\_thresh + d \cdot thresh\_depth\_gain \)
			</p>
			<p>
				Because of the noise in odometry and depth data, we improved the performance by post-processing the binary mask with OpenCV morphological operations.
				Opening removes small points of noise, while closing removes small holes in the mask.

				Once we have extracted a processed binary mask for the frame, we use scipy.ndimage.label to label connected components. Each connected component is extracted
				as a mask and assumed to be a dynamic object. The pixels in the mask are unprojected to get a 3D pointcloud representing the dynamic object. We discard 
				objects with a maximum axial variance outside a set range to further remove noise.
			</p>
			<p>
				To facilitate tracking, we compute frame-by-frame associations between currently tracked dynamic objects and newly detected dynamic objects.
			</p>
			<p>
				At each iteration, we predict the 3D location of currently tracked dynamic objects by propagating their masks into the next image frame using optical flow,
				then using the next frame's depth data to unproject the masks into predicted point clouds. When dynamic objects are detected in the next frame, we unproject their masks
				in that frame to get detected point clouds. We then use the Hungarian algorithm for global nearest neighbors with point cloud centroid distance-based similarity to match newly detected dynamic objects with 
				currently tracked dynamic objects using their associated point clouds. 
				<br><br>
				We augment the cost matrix and set costs to a large value if the point clouds are more than 
				a set distance apart, so that newly detected objects not close to any currently tracked objects are not matched and become new tracked objects. Otherwise, currently
				tracked objects are updated with their new detections.
			</p>
			<p>Representative videos of our algorithm running on the data we collected are shown below. Top left and right are the RGB and depth images. 
				Bottom right is the velocity residual magnitude, and bottom left is the post-processed binary mask. Overlaid on the images are the tracked 
				dynamic objects and their IDs.
				Noise is still present, but much of it is filtered out.
			</p>
			<div class="media-container">
				<video class='my-video' loop autoplay muted controls style="width: 800px; max-width: 100%;">
					<source src="./videos/hamilton.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 3. Clockwise from top left: RGB image, depth image, <br>
					velocity magnitude from flow residual, post-processed binary mask</span>
			</div>
			<br>
			<div class="media-container">
				<video class='my-video' loop autoplay muted controls style="width: 800px; max-width: 100%;">
					<source src="./videos/hamilton_two_robots.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 4. The same dynamic object maintains a consistent ID throughout frames where it is <br>
					moving continuously, showing the efficacy of the Hungarian algorithm for frame-to-frame association.
				</span>
			</div>
			</div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		    </div>
	</div>

	<div class="content-margin-container" id="experiments">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<hr style="margin-bottom: 30px">
		<h1>Experiments</h1>
		<h4>Dataset</h4>
            We evaluate our method on our own dataset, collected in the Highbay of the Aerospace Controls Laboratory at MIT. We used a RealSense D455 depth camera 
			on a Clearpath Husky with an Ouster lidar and MicroStrain IMU running lidar-inertial odometry. The moving objects are a hexcopter and another ground robot.
			All robots were teleoperated. The ground truth pose data was collected using a Vicon motion capture system and recorded in a ROS2 bag. The Husky recorded 
			a ROS1 bag with depth camera and pose data. We use our own ROS bag processing software for dataloading.
			<br><br>
			The video of our algorithm running on the entire dataset is linked here: <a href="/dynamic-object-detection/videos/hamilton_full.mp4" target="_blank">[hamilton full demo]</a>.

			<h4>Metrics</h4>

			We run our algorithm on the entire dataset. For each frame of the output, if there is ground truth data close to the associated timestamp, we compute the 
			following metrics:

			<ul>
				<li>Theoretical number of dynamic objects according to ground truth data. (\( N_{gt} \))</li>
					<ul>
						<li>Since we have ground truth poses, we can compute where the Scout and hexcopter are in the current image frame.
							We can also approximate their velocities by linearizing around their poses at that frame (0.25s ahead/behind).
							If the centroid of a robot is within the image dimensions and it has an approximate velocity 
							above the depth-scaled threshold from the parameters, we count it as a dynamic object in that frame.</li>
					</ul>
				<li>Number of detected ground truth dynamic objects (\( N_{det} \))</li>
					<ul>
						<li>If there is a detected object in that frame with a point cloud centroid within 1.0m* of the ground truth dynamic object pose, 
							we count that object as detected in that frame.
						</li>
						<li>This also gives us the number of ground truth dynamic objects not detected; i.e., false negative (\( N_{missed} \))</li>
					</ul>
				<li>Total number of detected dynamic objects (\( N \))</li>
				<li>Number of correctly detected dynamic objects. (\( N_{corr} \))</li>
					<ul>
						<li>
							If the detected object's point cloud centroid is within 1.0m of the ground truth object pose, we count it as a correct detection.
						</li>
						<li>
							This also gives us the number of detected dynamic objects not associated with ground truth data; i.e., false positives (\( N_{incorr} \)).
						</li>
					</ul>
			</ul>

			<i>* Our robots are ~1.0m in their largest dimension, so we used that as a reasonable threshold. The ground truth data is 
			not aligned with the robot center of mass in the robot Z axis, creating steady state error. Although far from a perfect 
			metric, this should be a reasonable estimate and allow us to compare runs with different parameters.</i> <br><br>


			We also define the following percentages for ease of comparison:
			<ul>
				<li>Percentage of ground truth dynamic objects detected \( det\% =\dfrac{N_{det}}{N_{gt}}\)</li>
				<li>Percentage of false positives \( fp\% =\dfrac{N_{incorr}}{N}\)</li>
			</ul>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="results">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
		<h1>Results</h1>
		We evaluated the performance of our algorithm when certain parameters are changed, while all others are kept constant. 
		First, we determined the best RAFT model for our application. RAFT options were not touched, besides defining iter=12 for each model.

		<div class="caption">
			<span>Table 1. RAFT model comparison</span>
		</div>
		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Model</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>raft-kitti</td>
					<td>1283</td>
					<td>1041</td>
					<td>1416</td>
					<td>1058</td>
					<td>81.14%</td>
					<td>25.28%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>raft-things</td>
					<td>1283</td>
					<td>1085</td>
					<td>1383</td>
					<td>1110</td>
					<td>84.57%</td>
					<td>19.74%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td><strong>raft-sintel</strong></td>
					<td>1283</td>
					<td>1088</td>
					<td>1384</td>
					<td>1114</td>
					<td><strong>84.80%</strong></td>
					<td><strong>19.51%</strong></td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>raft-chairs</td>
					<td>1283</td>
					<td>672</td>
					<td>1173</td>
					<td>702</td>
					<td>52.38%</td>
					<td>40.15%</td>
				</tr>
			</tbody>
		</table>
		<br>
		Subsequent evaluations were run with the raft-sintel model.
		<br><br>
		We then evaluated the effect of framerate on our algorithm. The original RGBD data was recorded at 30Hz,
		but we downsampled it to 15, 10, and 6 Hz.
		
		<div class="caption">
			<span>Table 2. Framerate comparison</span>
		</div>

		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Framerate</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td><strong>15Hz</strong></td>
					<td>1283</td>
					<td>1088</td>
					<td>1384</td>
					<td>1114</td>
					<td><strong>84.80%</strong></td>
					<td>19.51%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>10Hz</td>
					<td>849</td>
					<td>715</td>
					<td>900</td>
					<td>731</td>
					<td>84.22%</td>
					<td><strong>18.78%</strong></td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>6Hz</td>
					<td>513</td>
					<td>416</td>
					<td>530</td>
					<td>422</td>
					<td>81.09%</td>
					<td>20.38%</td>
				</tr>
			</tbody>
		</table>
		<br>
		The relative performance of the algorithm is similar across framerates, but a higher framerate allows for earlier 
		detection of dynamic objects. Therefore, we ran all other evaluations at 15Hz.

		<br><br>
		We then evaluated the effect of various noise-reduction methods by performing ablations on various 
		parameters of the algorithm.

		<div class="caption">
			<span>Table 3. Noise reduction comparison</span>
		</div>

		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Method</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td><strong>Original</strong></td>
					<td>1283</td>
					<td>1088</td>
					<td>1384</td>
					<td>1114</td>
					<td>84.80%</td>
					<td><strong>19.51%</strong></td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>No Morphology</td>
					<td>1283</td>
					<td>1102</td>
					<td>2626</td>
					<td>1294</td>
					<td>85.89%</td>
					<td>50.72%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>No axial variance-based threshold</td>
					<td>1283</td>
					<td>1119</td>
					<td>1662</td>
					<td>1183</td>
					<td><strong>87.22%</strong></td>
					<td>28.82%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>No depth-based velocity threshold gain*</td>
					<td>1318</td>
					<td>1097</td>
					<td>1605</td>
					<td>1112</td>
					<td>83.23%</td>
					<td>30.72%</td>
				</tr>
			</tbody>
		</table>
		<br>
		<i>* We set the fixed velocity threshold 
		to 0.775 (0.4 + 0.125 * 3) for this run. This was previously the average velocity threshold in our depth range (6m),
		since the original parameters were 0.4 for the minimum velocity threshold and 0.125 for the depth-based gain.</i>
		<br><br>
		We find that morphological operations are very effective at removing noise from the mask. Variance-based discarding 
		and depth-based velocity threshold gain also have a noticeable effect. Variance-based thresholding and morphological operations 
		somewhat reduce the detection rate, but the reduction in false positives is more significant.

		<br><br>

		To further reduce false positives, we evaluated the effect of requiring that a dynamic object 
		be detected in two consecutive frames before being considered a dynamic object.

		<div class="caption">
			<span>Table 4. Using 3D residuals</span>
		</div>

		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Method</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>Original</td>
					<td>1283</td>
					<td>1088</td>
					<td>1384</td>
					<td>1114</td>
					<td><strong>84.80%</strong></td>
					<td>19.51%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td><strong>2-Consecutive Requirement</strong></td>
					<td>1283</td>
					<td>1080</td>
					<td>1224</td>
					<td>1093</td>
					<td>84.18%</td>
					<td><strong>10.70%</strong></td>
				</tr>
			</tbody>
		</table>
		<br>

		This change halved the number of false positives and barely decreased the detection rate, though we did not 
		include it in the default algorithm used for the other evaluations, since we did not want to influence results 
		by relying on the probabilistic assumption that noise is independent between frames.

		<br><br>

		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Conclusion</h1>
						Although our method shows promise in efficient real-time detection and tracking of dynamic obstacles, it is extremely sensitive to the quality of data. 
						During our experiments, we observed that poor quality depth data or odometry estimates led to the geometric optical flow having large deviations
						from the RAFT optical flow in static areas of the image. Future work could include using a trained neural network to segment dynamic objects, since we do not
						use the corresponding RGB image as a prior for the segmentation of flow residuals. A neural network that also uses FastSAM segments or superpixels
						would better leverage all the available data. 
						<br><br>
						A planned extension of this work is to use LiDAR as the source for depth data. This would greatly increase the range and accuracy of depth data, at
						the cost of sparsity (which could be addressed with stereo camera-LiDAR fusion). In addition, it may allow us to use the 3D residual method discussed in <a href="#app1">Appendix I</a>,
						which can better detect movement in the camera Z axis.
						<br><br>
						A major limitation of this method is that under high accelerations, the RAFT and geometric flow significantly diverge, leading to underwhelming results during UAV flights. 
						There are multiple culprits, including noisy depth data and poor RAFT results at high accelerations. The most likely reason seems to be poor odometry,
						since the divergence often occurs during acceleration, suggesting latency and/or noise in the odometry data. Also, since pose data is collected at independent 
						times from the RGBD data, we must perform linear interpolation to approximate it every frame, which introduces additional error.
						<br><br>
						Despite these shortcomings, our method proves to be a promising approach to the problem of dynamic obstacle tracking. With only odometry and depth camera data, 
						it demonstrates good performance and runtime on real-world data in the tested environments. There appear to be many directions for further work to
						improve robustness, efficiency, and generalization.
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<hr style="margin-bottom: 30px">
							<h1>References</h1>
							<a id="ref_1"></a>[1] <a href="https://ieeexplore.ieee.org/abstract/document/8968021">Tordesillas, J., Lopez, B. T., & How, J. P. (2019, November). Faster: Fast and safe trajectory planner for flights in unknown environments. In 2019 IEEE/RSJ international conference on intelligent robots and systems (IROS) (pp. 1934-1940). IEEE.</a><br><br>
							<a id="ref_2"></a>[2] <a href="https://ieeexplore.ieee.org/abstract/document/9490372	">Tordesillas, J., & How, J. P. (2021). MADER: Trajectory planner in multiagent and dynamic environments. IEEE Transactions on Robotics, 38(1), 463-476.</a><br><br>
							<a id="ref_3"></a>[3] <a href="https://ieeexplore.ieee.org/abstract/document/10161244">Kondo, K., Tordesillas, J., Figueroa, R., Rached, J., Merkel, J., Lusk, P. C., & How, J. P. (2023, May). Robust MADER: Decentralized and asynchronous multiagent trajectory planner robust to communication delay. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1687-1693). IEEE. </a><br><br>
							<a id="ref_4"></a>[4] <a href="https://ieeexplore.ieee.org/abstract/document/9721028">Tordesillas, J., & How, J. P. (2022). Panther: Perception-aware trajectory planner in dynamic environments. IEEE Access, 10, 22662-22677. </a><br><br>
							<a id="ref_5"></a>[5] <a href="https://www.science.org/doi/full/10.1126/scirobotics.aaz9712">Falanga, D., Kleber, K., & Scaramuzza, D. (2020). Dynamic obstacle avoidance for quadrotors with event cameras. Science Robotics, 5(40), eaaz9712. </a><br><br>
							<a id="ref_6"></a>[6] <a href="https://ieeexplore.ieee.org/abstract/document/6564752">Chao, H., Gu, Y., & Napolitano, M. (2013, May). A survey of optical flow techniques for UAV navigation applications. In 2013 International Conference on Unmanned Aircraft Systems (ICUAS) (pp. 710-716). IEEE. </a><br><br>
							<a id="ref_7"></a>[7] <a href="https://link.springer.com/chapter/10.1007/978-3-031-73033-7_2">Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., & Rupprecht, C. (2024, September). Cotracker: It is better to track together. In European Conference on Computer Vision (pp. 18-35). Cham: Springer Nature Switzerland. </a><br><br>
							<a id="ref_7"></a>[7] <a href="https://link.springer.com/chapter/10.1007/978-3-031-73033-7_2">Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., & Rupprecht, C. (2024, September). Cotracker: It is better to track together. In European Conference on Computer Vision (pp. 18-35). Cham: Springer Nature Switzerland. </a><br><br>
							<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2404.15259">Smith, C., Charatan, D., Tewari, A., & Sitzmann, V. (2024). Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent. arXiv preprint arXiv:2404.15259.</a><br><br>
							<a id="ref_9"></a>[9] <a href="https://link.springer.com/chapter/10.1007/978-3-030-58536-5_24">Teed, Z., & Deng, J. (2020). Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16 (pp. 402-419). Springer International Publishing. </a><br><br>
							<a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/abstract/document/9686955">Tian, Y., Chang, Y., Arias, F. H., Nieto-Granda, C., How, J. P., & Carlone, L. (2022). Kimera-multi: Robust, distributed, dense metric-semantic slam for multi-robot systems. IEEE Transactions on Robotics, 38(4). </a><br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
	</div>

	<div class="content-margin-container" id="acknowledgements">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
					<h1>Acknowledgments</h1>
					<p>
						We would like to thank Professor Jonathan How for his guidance throughout this project and for 
						generously providing access to the resources that we utilized. Special thanks as well to Lucas Jia 
						for assisting us with physical experiments and data collection.
					</p>
		</div>
		<div class="margin-right-block">
		</div>
	</div>


	<div class="content-margin-container" id="codebase">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<hr style="margin-bottom: 30px">
						<h1>Codebase</h1>
						<p>See <a href="https://github.com/AndyLi23/dynamic-object-detection" target="_blank">[dynamic-object-detection on Github]</a> for our final implementation code. 
							Instructions for downloading our data and running the algorithm and evaluations are in the README. </p>
						<p>The prototype code is also <a href="https://github.com/jrached/cv_project">[hosted on Github]</a>.</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="appendix">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
					<hr style="margin-bottom: 30px">
					<h1>Appendix</h1>

					<h4 id="app1">I. Using 3D Residuals </h4>

					Because we have the depth data for both frame 1 and frame 2, in theory we can compute the full 3D movement 
					of each pixel between frames.
					Given RAFT flow \( [\delta u_{raft}, \delta v_{raft}] \) at pixel \( [u, v] \), we can compute the 3D residual as follows:
					\[ p_2 = (T_{odom}^{cam})^{-1} \cdot T_{r2}^{r1} \cdot T_{odom}^{cam} \cdot [p_1^T \dots 1]^T + [u_{raft}, v_{raft}, 0]^T \]
					\[ p_{2, raft} = d_2 K^{-1} [u + \delta u_{raft}, v + \delta v_{raft}, 1]^T \]
					\[ \delta \approx p_{2, raft} - p_2 \]

					where \( d_2 \) is the depth of pixel \( [u + \delta u_{raft}, v + \delta v_{raft}] \) in frame 2. This method works by 
					assuming that RAFT correctly predicts pixel correspondences, in which case the 3D point corresponding to pixel 
					\( [u + \delta u_{raft}, v + \delta v_{raft}] \) in frame 2 is the same as the 3D point corresponding to pixel \( [u, v] \) in frame 1.
					Since \( p_2 \) accounts for the camera motion between frames but not the motion of the point itself, while RAFT accounts
					for both, the difference is the approximate 3D motion of the point between frames.
					<br><br>
					Unfortunately, this method is not effective in practice. The RAFT flow is not accurate enough, 
					and noise in the depth data is amplified by the projection into 3D. We ran our evaluation with the 3D 
					residual method and found that it decreased the detection rate and dramatically increased the 
					number of false positives, mostly patches of depth noise at larger depths.

					<div class="caption">
						<span>Table 4. Using 3D residuals</span>
					</div>
			
					<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
						<thead>
							<tr>
								<th>Method</th>
								<th>\( N_{gt} \)</th>
								<th>\( N_{det} \)</th>
								<th>\( N \)</th>
								<th>\( N_{corr} \)</th>
								<th>\( det\% \)</th>
								<th>\( fp\% \)</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td><strong>Original</strong></td>
								<td>1283</td>
								<td>1088</td>
								<td>1384</td>
								<td>1114</td>
								<td><strong>84.80%</strong></td>
								<td><strong>19.51%</strong></td>
							</tr>
						</tbody>
						<tbody>
							<tr>
								<td>3D residual</td>
								<td>1283</td>
								<td>939</td>
								<td>35857</td>
								<td>1400</td>
								<td>73.19%</td>
								<td>96.10%</td>
							</tr>
						</tbody>
					</table>
					<br>

					<h4 id="app2">II. Max Depth Limits</h4>

					We later compared performances at different maximum depth limits (all other parameters fixed).

					<div class="caption">
						<span>Table 5. Depth limit comparison</span>
					</div>
			

					<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
						<thead>
							<tr>
								<th>Depth Limit (m)</th>
								<th>\( N_{gt} \)</th>
								<th>\( N_{det} \)</th>
								<th>\( N \)</th>
								<th>\( N_{corr} \)</th>
								<th>\( det\% \)</th>
								<th>\( fp\% \)</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td><strong>6.0</strong></td>
								<td>1283</td>
								<td>1088</td>
								<td>1384</td>
								<td>1114</td>
								<td><strong>84.80%</strong></td>
								<td>19.51%</td>
							</tr>
						</tbody>
						<tbody>
							<tr>
								<td>7.5</td>
								<td>1306</td>
								<td>1087</td>
								<td>1434</td>
								<td>1113</td>
								<td>83.23%</td>
								<td>22.38%</td>
							</tr>
						</tbody>
						<tbody>
							<tr>
								<td>10.0</td>
								<td>1306</td>
								<td>1049</td>
								<td>1317</td>
								<td>1075</td>
								<td>80.32%</td>
								<td>28.82%</td>
							</tr>
						</tbody>
						<tbody>
							<tr>
								<td>15.0</td>
								<td>1306</td>
								<td>1042</td>
								<td>1288</td>
								<td>1068</td>
								<td>79.79%</td>
								<td><strong>17.08%</strong></td>
							</tr>
						</tbody>
					</table>
					<br>
					As the maximum depth limit increases, the number of dynamic objects according to the ground truth 
					does not significantly increase (possibly due to the depth-based velocity threshold gain). The 
					detection rate decreases with increased depth limit.
					<br><br>
					We looked at the output videos and found that 15.0m decreased the false positive rate because it 
					brought the walls into range of the depth data,
					so odometry errors caused larger portions of the image to be masked, which were then pruned by the 
					axis variance threshold. However, this is not a generalizable phenomenon. Therefore, we decided to 
					continue using the 6.0m limit suggested by Intel, as any depth beyond that is guaranteed to be noisy. 
					<br><br>

					<h4>III. Smoothing and Quadcopter Experiments</h4>
					While developing the prototype, we noticed rapid changes in pose produced high-frequency noise in the flow residuals, 
					especially for quadcopter experiments where video was recorded on a highly dynamic vehicle. 
					Our first instinct was to design a low-pass filter to smooth the outputs of our pipeline. Thus, our prototype returned a 
					running average of the flow residuals parametrized by \( \alpha \) as 
					\[ smoothFlow =  \alpha \cdot Flow + (1 - \alpha) \cdot prevFlow \]
					As expected of a running average, smoothing mitigated some of the noise, but it also generated a delay between the estimated and ground truth
					obstacle position. In fact, some smoothing granted a minor performance boost, but it quickly degraded as we over-smoothed the signal and 
					created a larger gap between the estimate and the actual position. 
					
					<img src="./images/prototype/pos_error.png" width=1028px/>
					<br>
					<img src="./images/prototype/pos_std.png" width=1028px/>
					<br>

					This motivated us to pivot from the running average approach. The analysis above was performed on data collected onboard a x500 Holybro Quadcopter. 
					Below are the flow masks of the flow from the videos recorded by the quadcopter with \(1 - \alpha \) = 0.2 smoothing (left) and  \(1 - \alpha \) = 0.8 smoothing
					(right). The flashes of white are moments in which the RAFT flow and geometric flow disagree. The attentive viewer will notice that the flashing occurs when the 
					quadcopter rolls. We found this to be a common trend: the signal-to-noise ratio decreases as the vehicle performs aggressive maneuvers, particularly during sharp 
					changes in orientation. Notice that the video on the right exhibits less flashing, as it is heavily smoothed. This comes at the expense of a 
					higher trajectory error as the smoothed estimate lags behind the unprocessed one. Although we developed an improved noise reduction strategy, the noise introduced 
					during aggressive UAV flight requires advanced denoising techniques beyond the scope of this project, so we constrained the remainder of our experiments to ground vehicles. 
					<br><br>
					<div class="media-container">
						<video class='my-video' loop autoplay muted controls style="width: 300px; max-width: 50%;">
							<source src="./videos/prototype/flow_unsmoothed.mp4" type="video/mp4">
						</video>
						<video class='my-video' loop autoplay muted controls style="width: 300px; max-width: 50%;">
							<source src="./videos/prototype/flow_smoothed.mp4" type="video/mp4">
						</video>
					</div>
					<div class="caption">
						<span>Figure 5. Left is light smoothing, right is heavy smoothing.
						</span>
					</div>
					<br>
					To estimate the obstacle position, we unproject the points in the mask into 3D space and obtain their centroid.
					We use this centroid as our prediction of the obstacle's position. Clearly, a significant limitation of this approach is that it only works in scenes with at most one dynamic obstacle.  
					Note that these are preliminary results, meant to convey the challenge of handling noisy visual signals during aggressive motion. 
					Our final implementation computes the trajectory error by following a more sophisticated procedure outlined in the methods and experiment sections of this blog. 
					<br><br>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	</body>

</html>


