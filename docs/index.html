<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}

	.main-content-block {
		width: 80%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 30px 8px 30px;
		font-family: 'Garamond', serif;
		line-height: 1.5;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Garamond', serif;
			line-height: 1.5;
			padding: 5px;
	}
	.margin-right-block {
			font-family: 'Garamond', serif;
			line-height: 1.5;
			font-size: 14px;
			width: 5%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.caption {
		text-align: center; 
		margin-top: 10px;
		font-size: 16px;
		font-style: italic;
	}

	video {
		margin: 0;
	}

	.media-container {
		display: flex; 
		justify-content: center; 
		align-items: center; 
		gap: 0;	
	}
	



</style>

	  <title>FLOROS</title>
      <meta property="og:title" content="FLOROS" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Garamond', serif;">Flow Residuals for Open Set dynamic object tracking (FLOROS)</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:21px"><a href="your_website">Qingyuan Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:21px"><a href="your_partner's_website">Juan Rached</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">MIT 6.8300 Final Project</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#related_works">Related Works</a><br><br>
              <a href="#methods">Methods</a><br><br>
              <a href="#experiments">Experiments</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
              <a href="#citations">References</a><br><br>
			  <a href="#codebase">Codebase</a><br><br>
			  <a href="#appendix">Appendix</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
			<p>
				Advances in autonomous Unmanned Aerial Vehicle (UAV) navigation are improving our quality of life. The integration of this technology into emergency services, 
				search and rescue, first responders, as well as law-enforcement, and agriculture has observed accelerated growth in the past decade, promising a safer future.
				UAV navigation has also had an impact in the delivery industry. While this might be perceived as superfluous next to the former, UAV deliveries enable the prompt 
				dispatch of sensible products, such as medicine, to remote communities, difficult to access through ground transportation. This is still, however, an early technology,
				with several limitations and open research questions. In particular, dynamic obstacle avoidance for UAVs is a longstanding problem in robotics.
			</p>
			<p>
				Current state-of-the-art methods rely on event cameras, stereo cameras, depth cameras, or lidar.
				But power and payload limitations restrict the perception and compute capacity of a UAV. These constraints give rise to a hard-to-strike 
				balance between latency and performance for real-time applications that is still an open question in the field today. 
				Some methods favor efficiency while some favor performance, but the field is yet to adopt a general procedure to estimate the motion of dynamic obstacles. 
				As a consequence, a large number of path planning algorithms assume static environments, but this is a 
				strong assumption as we live in a dynamic world. Some methods perform dynamic obstacle avoidance by assuming the state of the dynamic obstacle is known at all times. 
				This is fine in a laboratory setting, where a motion capture system can provide such estimates, but breaks in real world applications. So a scheme for estimating the motion 
				of dynamic obstacles from sensory input is indispensable if we are to deploy a complete system. Some approaches employ Kalman Filters to recover the position and velocity of the obstacle in the field 
				of view from a point cloud. But this is either restrictive, allowing robust planning only around vehicles with known dynamics, or brittle, if nothing is assumed about the obstacles' dynamics. 
			</p>
			<p>
				There are two core challenges in estimating the motion of objects from a moving platform:
				<ol>
					<li>Estimating the motion of the scene.</li>
					<li>Removing the motion of the vehicle attached to the camera.</li>	
				</ol>
			</p>
			<p>
				This becomes increasingly difficult when performing highly dynamic, high-speed motion as is the norm in the UAV literature. At these speeds,
				motion blur pollutes the visual signal, which complicates the task of separating agent from obstacle motion. In this project, we propose the use of RAFT optical flow and the optical 
				flow induced by scene flow to solve both of the aforementioned problems with just a frame camera.
			</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="related_works">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
					<h1>Related Work</h1>
		<p>
			In <a href="#ref_1">[1]</a>, Tordesillas et al, propose a fast and safe trajectory planner that plans in known and unknown environments. In other words, it plans within the observed and 
			unobserved regions of the map. But this approach assumes a static world. In <a href="#ref_2">[2]</a> and <a href="#ref_3">[3]</a> present a multiagent, asynchronous, and decentralized trajectory
			planner and an extension to mitigate delay checks in hardware deployment. While both <a href="#ref_2">[2]</a> and <a href="#ref_3">[3]</a> can plan in dynamic environments, they assume that the state of the 
			dynamic objects is known. <a href="#ref_4">[4]</a>, a real-time perception-aware trajectory planner for multirotor UAVs, can plan in dynamic environments and does not expect 
			the state of dynamic objects to be given. Instead, <a href="#ref_4">[4]</a> clusters the point clouds produced by an onboard depth sensor and applies the Hungarian algorithm to track 
			those clusters. <a href="#ref_4">[4]</a>, however, does not work in static scenes, and its point cloud scheme breaks when static objects are picked up by the depth sensor as well. 
		</p>
		
		<p>		
			<a href="#ref_5">[5]</a> propose the use of event cameras for low-latency dynamic obstacle detection and avoidance.
			While they achieve faster response times than frame cameras, event cameras are expensive and do not capture the full scene. 
			Since they cannot pick up static objects or color, they are incapable of performing tasks relevant to robotics systems such as feature matching, segmentation, among others.
			Frame cameras can be used to solve a variety of perception tasks, but require more elaborate algorithms for motion detection than event cameras. 
		</p>


		<p>
			The fundamental vision-based approach to motion estimation is optical flow, and optical flow methods for UAV applications have been thoroughly studied <a href="#ref_6">[6]</a>. 
			But standard optical flow struggles with fine motion estimation, particularly at high speeds and in dynamic lighting conditions, making them inadequate for UAV navigation. Additionally, 
			pure optical flow methods estimate motion between frames, but capture no semantics about the motion and its corresponding obstacle and so struggle to track the objects' trajectory along a sequence. 
			<a href="#ref_7">[7]</a> presents a transformer-based point tracking pipeline that successfully estimates the trajectory of dynamic objects from medium-to-long video sequences. 
			Estimating obstacle trajectories with point tracking is considerably more robust than with optical flow alone, but <a href="#ref_7">[7]</a> needs a procedure to remove the ego motion of the UAV from the point tracks. 	
		</p>

		<p>
			<a href="#ref_8">[8]</a> leverages the scene flow induced by a moving camera to produce high-quality camera poses, intrinsics and depth estimates. 
			We propose reversing the pipeline from <a href="#ref_8">[8]</a>, computing the expected optical flow between two frames given depth data, camera poses, and intrinsics, under the assumption of a static scene. 
			If there are any non-static objects in the scene, we can subtract the expected optical flow from the RAFT-estimated optical flow, <a href="#ref_9">[9]</a> , to obtain an estimate of the motion of the scene not arising from ego motion.
			We will develop an algorithm to convert the residuals into a discrete list of moving objects and their masks. We propagate the masks between frames to obtain the tracks for the dynamic obstacles over time. 

		</p>
		
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Methods</h1>

			<p>
				We compute the residuals from RAFT optical flow and the geometric optical flow induced by scene flow, using solely RGBD and odometry data. The latter assumes a static scene, so 
				the residuals substract the flow produced by the motion of the vehicle, capturing only the movement of the dynamic objects in the scene. Below is a 
				detailed discussion on how each of the modules involved in our approach work and the role they play in the pipeline. 
			</p>

			<h4>Theory</h4>
			<p>
				Optical flow assigns pixel correspondences between two frames by solving the optimization problem below: 
			</p>
			\[ E = \min_{\mu} \left[ (\nabla^T \mathbf{I}_1\mu - (\mathbf{I}_2-\mathbf{I}_1)^2+\alpha(|\nabla \mu_x|^2+|\nabla \mu_y|^2)) \right] \] 

			<p>
				Where the first term enforces correspondences based on the color constancy assumption, which states that the color of each point in the scene does not change between frames, and the second term promotes smooth uniform flow by encouraging each 
				flow vector to be close to the average of its neighboring vectors. Common pitfalls of classic optical flow include: 
			</p>
			<ol>
				<li>It is expensive to solve an optimization problem for each frame in a video sequence.</li>
				<li>The color constancy assumption is susceptible to blur and changes in lighting conditions.</li>
			</ol>
			<p>
				As a consequence, it has become the norm to substitute the classical optical flow approach with learned models, the most notable of which is 
				RAFT. RAFT extracts per pixel features and builds multi-scale 4D correlation volumes. This rich representation allows the model to generalize 
				and reduce the effect of lighting changes and blur on the flow estimates. The compact representation makes it so that computing the flow between 
				each frame is also faster than if solving the original optimization problem. 
			</p>
			<p>
				We obtain our flow residuals by computing the difference between the RAFT optical flow and the "geometric optical flow", or optical flow
				resulting solely from camera-induced scene flow. 
				<br><br>
				For each pixel \( [u, v] \) with corresponding depth \( d \) in the first frame, we first unproject the pixel coordinates into 3D space using the depth data and camera intrinsics:
				\[ p_1=d K^{-1} [u, v, 1]^T \]
				We assume that \( p_1 \) is static in the 3D scene. We transform the 3D point \( p_1 \), which is in the reference frame of the camera at the frame 1, into the reference frame of the camera at the frame 2. Given that 
				the robot pose is \( T_{world}^{r1} \) at frame 1 and \( T_{world}^{r2} \) at frame 2, and that the transform between the odometry frame and camera frame is \( T_{odom}^{cam} \), 
				we can transform the point as follows:

				\[ T_{r2}^{r1} = (T_{world}^{r2})^{-1} \cdot T_{world}^{r1} \]
				\[ p_2 = (T_{odom}^{cam})^{-1} \cdot T_{r2}^{r1} \cdot T_{odom}^{cam} \cdot [p_1^T \dots 1]^T \]

				Finally, we project the point \( p_2 \) into the image plane to get its pixel location in the second frame:
				\[ [u', v'] = [f_x \frac{p_{2,x}}{p_{2,z}} + c_x, f_y \frac{p_{2,y}}{p_{2,z}} + c_y] \]
				Where \( f_x, f_y \) are the focal lengths of the camera and \( c_x, c_y \) are the coordinates of the camera center in the image plane.

				The \( [u', v'] - [u, v] \) gives us the geometric optical flow at \( [u, v] \) in frame 1.

				The key observation is the assumption that \( p_1 \) is static in the 3D scene. If \( p_1 \) moves \( \delta = [\delta_x, \delta_y, \delta_z] \) (in the camera reference frame at frame 2) 
				between frame 1 and frame 2, we get the following image projection at frame 2:

				\[ [u'_{dynamic}, v'_{dynamic}] = [f_x \frac{p_{2,x} + \delta_x}{p_{2,z} + \delta_z} + c_x, f_y \frac{p_{2,y} + \delta_y}{p_{2,z} + \delta_z} + c_y] \]

	
				In the video below, RAFT optical flow is juxtaposed with geometric optical flow. 
				Where the geometric optical flow is white, it cannot be computed because there is no depth data (we capped the
				max depth at 6.0m, as suggested by Intel).
			</p>
			<div class="media-container">
				<video class='my-video' loop autoplay muted style="width: 800px; max-width: 100%;">
					<source src="./videos/flow.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 1. Clockwise from top left: RGB image, depth image, <br>geometric optical flow, RAFT flow</span>
			</div>
			<p>

				Since RAFT estimates the actual observed optical flow from frame 1 to frame 2, it will return an optical flow close to \( [u'_{dynamic}, v'_{dynamic}] - [u, v] \) at \( [u, v] \).
				Then, the residual (RAFT minus geometric flow) at that pixel is \( [u'_{dynamic}, v'_{dynamic}] - [u', v'] \).

				Although we could theoretically recover the depth of \( [u'_{dynamic}, v'_{dynamic}] \) from the depth data for frame 2, in practice, noisy depth data and imperfect
				pixel associations make this a noisy process. Instead, we assume that the object maintains constant depth between frames (\( \delta_z = 0 \)) and use the residual to only recover \( [\delta_x, \delta_y] \):

				\[ [\delta_x, \delta_y] = p_{2,z} ([u'_{dynamic}, v'_{dynamic}] - [u', v']) \cdot [\frac{1}{f_x}, \frac{1}{f_y}] \] 

				This assumption works reasonably well in practice, although it is not effective at detecting objects moving along the camera Z axis.
				<br><br>
				Finally, given \( v=\| \delta_x, \delta_y \| \) as an estimate for the velocity of the 3D point corresponding to each pixel, we can mask areas of the image with
				\( v \) higher than a set threshold to obtain masks of dynamic objects in frame 1.

				
			</p>
			<h4>Prototype</h4>

			<p>Below is a video of our prototype of this method running on data from Kimera-Multi, <a href="#ref_10">[10]</a>, where the right image shows flow residual magnitudes masked by the velocity threshold. 
				To track the motion of dynamic objects across frames we first fed the residuals to the point tracking algorithm from, <a href="#ref_7">[7]</a>.
				However, this introduces unnecessary computation, as RAFT optical flow is accurate enough to estimate the motion of each obstacle between the current and next frame.
				The details are discussed below.</p>
			<p>
				We also prototyped a method which uses the 3D residual between the optical flow predicted pointcloud and the odometry predicted pointcloud instead. 
				It was significantly noisier, so we used flow residuals for our final method. See the appendix below for details.
			</p>
			<div class="media-container">
				<video class='my-video' loop autoplay muted style="width: 300px; max-width: 50%;">
					<source src="./videos/prototype/kimera/kimera_walk_og.mp4" type="video/mp4">
				</video>
				<video class='my-video' loop autoplay muted style="width: 300px; max-width: 50%;">
					<source src="./videos/prototype/kimera/walk_masked_pt.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 2. Top is RGB image, bottom is flow residual magnitude <br> masked by the velocity threshold.</span>
			</div>
			<h4>Final Algorithm</h4>
			<p>
				We implemented the above method in a pipeline that works with standard ROS bags as input data, with flow and residuals computed in batches using PyTorch tensors on GPU 
				for performance. Once velocity magnitudes are computed from residual flow, we apply a threshold to the velocity magnitudes to obtain a binary mask of dynamic objects.
				The threshold can be tuned. Because depth camera noise increases as depth increases, we set the threshold to scale by depth: \( thresh=min\_thresh + z \cdot thresh\_depth\_gain \)
			</p>
			<p>
				Because of the noise in odometry and depth data, we improved the performance by post-processing the binary mask with OpenCV morphological operations.
				Opening removes small points of noise, while closing removes small holes in the mask.

				Once we have extracted a processed binary mask for the frame, we use scipy.ndimage.label to label connected components. Each connected component is extracted
				as a mask and assumed to be a dynamic object. The pixels in the mask are unprojected to get a 3D pointcloud representing the dynamic object. We discard 
				objects with a maximum axial variance outside a set range to further remove noise.
			</p>
			<p>
				To facilitate tracking, we compute frame-by-frame associations between currently tracked dynamic objects and newly detected dynamic objects.
			</p>
			<p>
				At each iteration, we predict the 3D location of currently tracked dynamic objects by propogating their masks into the next image frame using optical flow,
				then using the next frame's depth data to unproject the masks into predicted point clouds. When dynamic objects are detected in the next frame, we unproject their masks
				in that frame to get detected point clouds. We then use the Hungarian algorithm with point cloud centroid distance-based similarity to match newly detected dynamic objects with 
				currently tracked dynamic objects using their associated point clouds. 
				<br><br>
				We augment the cost matrix and set costs to a large value if the point clouds are more than 
				a set distance apart, so that newly detected objects not close to any currently tracked objects are not matched and become new tracked objects. Otherwise, currently
				tracked objects are updated with their new detections.
			</p>
			<p>A representative video of our algorithm running on data we collected is shown below. Top left and right are the RGB and depth images. 
				Bottom right is the velocity residual magnitude, and bottom left is the post-processed binary mask. Overlaid on the images are the tracked 
				dynamic objects and their IDs.
				Noise is still present, but much of it is filtered out.
			</p>
			<div class="media-container">
				<video class='my-video' loop autoplay muted style="width: 800px; max-width: 100%;">
					<source src="./videos/hamilton.mp4" type="video/mp4">
				</video>
			</div>
			<div class="caption">
				<span>Figure 3. Clockwise from top left: RGB image, depth image, <br>
					velocity magnitude from flow residual, post-processed binary mask</span>
			</div>
			</div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
		    </div>
	</div>

	<div class="content-margin-container" id="experiments">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
		<h1>Experiments</h1>
		<h4>Dataset</h4>
            We evaluate our method on our own dataset, collected in the Highbay of the Aerospace Controls Laboratory at MIT. We used a Realsense D455 depth camera 
			on a Clearpath Husky with an Ouster lidar and MicroStrain IMU running lidar-inertial odometry. The moving objects are a hexcopter and another ground robot.
			All robots were teleoperated. The ground truth pose data was collected using a Vicon motion capture system and recorded in a ROS2 bag. The Husky recorded 
			a ROS1 bag with depth camera and pose data. We use our own rosbag processing software for dataloading.
			<br><br>
			The video of our algorithm running on the entire dataset is linked here: <a href="/dynamic-object-detection/videos/hamilton_full.mp4" target="_blank">hamilton full demo</a>.

			<h4>Metrics</h4>

			We run our algorithm on the entire dataset. For each frame of the output, if there is ground truth data close to the associated timestamp, we compute the 
			following metrics:

			<ul>
				<li>Theoretical number of dynamic objects according to ground truth data. (\( N_{gt} \))</li>
					<ul>
						<li>Since we have ground truth poses, we can compute where the Scout and hexcopter are in the current image frame.
							We can also approximate their velocities by linearizing around their poses at that frame (0.25s ahead/behind).
							If the centroid of a robot is within the image dimensions and it has an approximate velocity 
							above the depth-scaled threshold from the parameters, we count it as a dynamic object in that frame.</li>
					</ul>
				<li>Number of detected ground truth dynamic objects (\( N_{det} \))</li>
					<ul>
						<li>If there is a detected object in that frame with a point cloud centroid within 1.0m* of the ground truth dynamic object pose, 
							we count that object as detected in that frame.
						</li>
						<li>This also gives us the number of ground truth dynamic objects not detected; i.e. false negative (\( N_{missed} \))</li>
					</ul>
				<li>Total number of detected dynamic objects (\( N \))</li>
				<li>Number of correctly detected dynamic objects. (\( N_{corr} \))</li>
					<ul>
						<li>
							If the detected object's point cloud centroid is within 1.0m of the ground truth object pose, we count it as a correct detection.
						</li>
						<li>
							This also gives us the number of detected dynamic objects not associated with ground truth data; i.e. false positives (\( N_{incorr} \)).
						</li>
					</ul>
			</ul>

			* Our robots are ~1.0m in their largest dimension so that seemed like a reasonable threshold. The ground truth data is 
			also not at the robot center of mass in the robot Z axis. <br><br>


			We also define the following percentages for ease of comparison:
			<ul>
				<li>Percentage of ground truth dynamic objects detected \( det\% =\dfrac{N_{det}}{N_{gt}}\)</li>
				<li>Percentage of false positives \( fp\% =\dfrac{N_{incorr}}{N}\)</li>
			</ul>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="results">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
		<h1>Results</h1>
		We evaluated our algorithm with different parameters. First, we determined the best RAFT model for our application. Each was run
		with all other parameters fixed. RAFT options were not touched, besides defining iter=12 for each model.

		<div class="caption">
			<span>Table 1. RAFT model comparison</span>
		</div>
		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Model</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>raft-kitti</td>
					<td>1283</td>
					<td>1041</td>
					<td>1416</td>
					<td>1058</td>
					<td>81.14%</td>
					<td>25.28%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>raft-things</td>
					<td>1283</td>
					<td>1085</td>
					<td>1383</td>
					<td>1110</td>
					<td>84.57%</td>
					<td>19.74%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td><strong>raft-sintel</strong></td>
					<td>1283</td>
					<td>1088</td>
					<td>1384</td>
					<td>1114</td>
					<td><strong>84.80%</strong></td>
					<td><strong>19.51%</strong></td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>raft-chairs</td>
					<td>1283</td>
					<td>672</td>
					<td>1173</td>
					<td>702</td>
					<td>52.38%</td>
					<td>40.15%</td>
				</tr>
			</tbody>
		</table>
		<br>
		Subsequent evaluations were run with the raft-sintel model.
		<br><br>
		We then evaluated the effect of framerate on our algorithm. The original RGBD data was recorded at 30Hz,
		but we downsampled it to 15, 10, and 6Hz and ran the algorithm with other parameters fixed. The results are shown below.
		
		<div class="caption">
			<span>Table 2. Framerate comparison</span>
		</div>

		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Framerate</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td><strong>15Hz</strong></td>
					<td>1283</td>
					<td>1086</td>
					<td>1380</td>
					<td>1111</td>
					<td><strong>84.65%</strong></td>
					<td>19.49%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>10Hz</td>
					<td>849</td>
					<td>715</td>
					<td>900</td>
					<td>731</td>
					<td>84.22%</td>
					<td><strong>18.78%</strong></td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>6Hz</td>
					<td>513</td>
					<td>416</td>
					<td>530</td>
					<td>422</td>
					<td>81.09%</td>
					<td>20.38%</td>
				</tr>
			</tbody>
		</table>
		<br>
		The relative performance of the algorithm is similar across framerates, but a higher framerate allows for earlier 
		detection of dynamic objects. Therefore, we ran all other evaluations at 15Hz.

		<br><br>
		We then evaluated the effect of various noise-reduction methods. 

		<div class="caption">
			<span>Table 2. Noise reduction comparison</span>
		</div>

		<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			<thead>
				<tr>
					<th>Method</th>
					<th>\( N_{gt} \)</th>
					<th>\( N_{det} \)</th>
					<th>\( N \)</th>
					<th>\( N_{corr} \)</th>
					<th>\( det\% \)</th>
					<th>\( fp\% \)</th>
				</tr>
			</thead>
			<tbody>
				<tr>
					<td>Original</td>
					<td>1283</td>
					<td>1088</td>
					<td>1384</td>
					<td>1114</td>
					<td>84.80%</td>
					<td><strong>19.51%</strong></td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>No Morphology</td>
					<td>1283</td>
					<td>1102</td>
					<td>2626</td>
					<td>1294</td>
					<td>85.89%</td>
					<td>50.72%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>No axial variance-based discarding</td>
					<td>1283</td>
					<td>1119</td>
					<td>1662</td>
					<td>1183</td>
					<td><strong>87.22%</strong></td>
					<td>28.82%</td>
				</tr>
			</tbody>
			<tbody>
				<tr>
					<td>No depth-based velocity threshold gain</td>
					<td>1318</td>
					<td>1097</td>
					<td>1605</td>
					<td>1112</td>
					<td>83.23%</td>
					<td>30.72%</td>
				</tr>
			</tbody>
		</table>
		<br>
		We find that morphological operations are very effective at removing noise from the mask. Variance-based discarding 
		and depth-based velocity threshold gain also have some effect. Variance-based discarding and morphological operations 
		somewhat reduce the detection rate, but the reduction in false positives is more significant.

		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Conclusion</h1>
						Although our method shows promise in efficient real-time detection and tracking of dynamic obstacles, it is extremely sensitive to the quality of data. 
						During our experiments, we observed that poor quality depth data or odometry estimates led to the geometric optical flow having large deviations
						from the RAFT optical flow in static areas of the image. Future work could include using a trained neural network to segment dynamic objects, since we do not
						use the corresponding RGB image as a prior for segmentation of flow residuals. A neural network that also uses FastSAM segments or superpixels
						would better leverage all the available data. 
						<br><br>
						A planned extension to this work is to use LiDAR as the source for depth data. This would greatly increase the range and accuracy of depth data, at
						the cost of sparsity (which could be addressed with stereo camera-LiDAR fusion). In addition, it may allow us to use the 3D residual method discussed in the appendix,
						which can better detect movement in the camera Z axis.
						<br><br>
						A major limitation of this method is that under high accelerations, the RAFT and geometric flow significantly diverge, leading to underwhelming results during UAV flights. 
						This might be attributed to RAFT not generalizing well to high speed camera motion. Performing transfer learning on RAFT  with labeled high speed flight data 
						is a promising direction of future work. 
						<br><br>
						Despite these shortcomings, our method proves to be a promising approach to the problem of dynamic obstacle tracking. With only odometry and depth camera data, 
						it demonstrates good performance and runtime on real-world data in the tested environments. There appear to be many directions for further work to
						improve robustness, efficency, and generalization.
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<h1>References</h1>
							<a id="ref_1"></a>[1] <a href="https://ieeexplore.ieee.org/abstract/document/8968021">Tordesillas, J., Lopez, B. T., & How, J. P. (2019, November). Faster: Fast and safe trajectory planner for flights in unknown environments. In 2019 IEEE/RSJ international conference on intelligent robots and systems (IROS) (pp. 1934-1940). IEEE.</a><br><br>
							<a id="ref_2"></a>[2] <a href="https://ieeexplore.ieee.org/abstract/document/9490372	">Tordesillas, J., & How, J. P. (2021). MADER: Trajectory planner in multiagent and dynamic environments. IEEE Transactions on Robotics, 38(1), 463-476.</a><br><br>
							<a id="ref_3"></a>[3] <a href="https://ieeexplore.ieee.org/abstract/document/10161244">Kondo, K., Tordesillas, J., Figueroa, R., Rached, J., Merkel, J., Lusk, P. C., & How, J. P. (2023, May). Robust MADER: Decentralized and asynchronous multiagent trajectory planner robust to communication delay. In 2023 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1687-1693). IEEE. </a><br><br>
							<a id="ref_4"></a>[4] <a href="https://ieeexplore.ieee.org/abstract/document/9721028">Tordesillas, J., & How, J. P. (2022). Panther: Perception-aware trajectory planner in dynamic environments. IEEE Access, 10, 22662-22677. </a><br><br>
							<a id="ref_5"></a>[5] <a href="https://www.science.org/doi/full/10.1126/scirobotics.aaz9712">Falanga, D., Kleber, K., & Scaramuzza, D. (2020). Dynamic obstacle avoidance for quadrotors with event cameras. Science Robotics, 5(40), eaaz9712. </a><br><br>
							<a id="ref_6"></a>[6] <a href="https://ieeexplore.ieee.org/abstract/document/6564752">Chao, H., Gu, Y., & Napolitano, M. (2013, May). A survey of optical flow techniques for UAV navigation applications. In 2013 International Conference on Unmanned Aircraft Systems (ICUAS) (pp. 710-716). IEEE. </a><br><br>
							<a id="ref_7"></a>[7] <a href="https://link.springer.com/chapter/10.1007/978-3-031-73033-7_2">Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., & Rupprecht, C. (2024, September). Cotracker: It is better to track together. In European Conference on Computer Vision (pp. 18-35). Cham: Springer Nature Switzerland. </a><br><br>
							<a id="ref_7"></a>[7] <a href="https://link.springer.com/chapter/10.1007/978-3-031-73033-7_2">Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., & Rupprecht, C. (2024, September). Cotracker: It is better to track together. In European Conference on Computer Vision (pp. 18-35). Cham: Springer Nature Switzerland. </a><br><br>
							<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2404.15259">Smith, C., Charatan, D., Tewari, A., & Sitzmann, V. (2024). Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent. arXiv preprint arXiv:2404.15259.</a><br><br>
							<a id="ref_9"></a>[9] <a href="https://link.springer.com/chapter/10.1007/978-3-030-58536-5_24">Teed, Z., & Deng, J. (2020). Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16 (pp. 402-419). Springer International Publishing. </a><br><br>
							<a id="ref_10"></a>[10] <a href="https://ieeexplore.ieee.org/abstract/document/9686955">Tian, Y., Chang, Y., Arias, F. H., Nieto-Granda, C., How, J. P., & Carlone, L. (2022). Kimera-multi: Robust, distributed, dense metric-semantic slam for multi-robot systems. IEEE Transactions on Robotics, 38(4). </a><br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
	</div>

	<div class="content-margin-container" id="codebase">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Codebase</h1>
						<p>See <a href="https://github.com/AndyLi23/dynamic-object-detection" target="_blank">dynamic-object-detection on Github</a> for our final implementation code. 
							Instrutions for downloading our data and running the algorithm and evaluations are in the README. </p>
						<p>The prototype is <a href="https://github.com/jrached/cv_project">also hosted on Github</a>.</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
	</div>

	<div class="content-margin-container" id="appendix">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
					<h1>Appendix</h1>
					<h4>Smoothing and Quadcopter Experiments</h4>
					While developing the prototype, we noticed rapid changes in pose produced high frequency noise in the flow residuals. 
					Especially, for quadcopter experiments where video was recorded on a highly dynamic vehicle. 
					Our first instinct was to design a low-pass filter to smooth the outputs of our pipeline. Thus, our prototype returned a 
					running average of the flow residuals parametrized by \( \alpha \) as 
					\[ smoothFlow =  \alpha \cdot Flow + (1 - \alpha) \cdot prevFlow \]
					As expected of a running average, smoothing mitigated some of the noise, but it also generated a delay between the estimated and ground truth
					obstacle position. In fact, some smoothing granted a minor performance boost, but it quickly degraded as we over-smoothed the signal and 
					created a larger gap between the estimate and the actual position. 
					
					<img src="./images/prototype/pos_error.png" width=1028px/>
					<br>
					<img src="./images/prototype/pos_std.png" width=1028px/>
					<br>

					This motivated us to pivot from the running average approach. The analysis above was performed on data collected onboard a x500 Holybro Quadcopter. 
					Below are the flow masks of the flow from the videos recorded by the quadcopter with \(1 - \alpha \) = 0.2 smoothing (left) and  \(1 - \alpha \) = 0.8 smoothing
					(right). The flashes of white are moments in which the RAFT flow and geometric flow disagree. The attentive viewer will notice that the flashed occur when the 
					quadcopter rolls. We found this to be a common trend: the signal-to-noise ration decreases as the vehicle performs aggressive maneuvers, particularly, while 
					performing changes in orientation. Notice, that the video on the right exhibits less flashing, as it is heavily smoothed. This comes at the expensive of a 
					higher trajectory error as the smoothed estimate lags behind the unprocessed one. Although we developed an improved noise reduction strategy, the noise introduced 
					in aggressive UAV flights require advanced denoising techniques beyond the scope of this project, so we constrained the remainder of our experiments to ground vehicles. 
				
					<br>
					<div class="media-container">
						<video class='my-video' loop autoplay muted style="width: 300px; max-width: 50%;">
							<source src="./videos/prototype/flow_unsmoothed.mp4" type="video/mp4">
						</video>
						<video class='my-video' loop autoplay muted style="width: 300px; max-width: 50%;">
							<source src="./videos/prototype/flow_smoothed.mp4" type="video/mp4">
						</video>
					</div>
					<br>
					To estimate the obstacle position we unproject onto 3D space the points in the flow mask, and average those points to obtain the "centroid" of the masked points.
					We use this centroid as our prediction of the obstacle's position. Clearly, a significant limitation of this approach is that it only works in scenes with at most one dynamic obstacle.  
					Note these are preliminary results, meant to convey the challenge of handling noisy visual signals during aggressive motion. 
					Our final implementation, computes the trajectory error by following a more sophisticated procedure outlined in the methods and experiment sections of this blog. 
					<h4>Using 3D Residuals </h4>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	</body>

</html>


